# Self-Attention Attribution: Interpreting Information Interactions Inside Transformer
